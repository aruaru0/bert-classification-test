{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aruaru0/bert-classification-test/blob/main/transformers_japanese_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformersのテスト"
      ],
      "metadata": {
        "id": "xjakYud3-PRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install packages"
      ],
      "metadata": {
        "id": "DSKTytGX-UZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRJICAim-DMp"
      },
      "outputs": [],
      "source": [
        "# インストール後、ランタイムを再起動する必要あり！！\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "# pip install　accelerate -U などでインストールすると再起動が必要となる\n",
        "!pip install git+https://github.com/huggingface/accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語関係\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ],
      "metadata": {
        "id": "8Xdc8HMXAN2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データをロード（amazonのレビューデータ）"
      ],
      "metadata": {
        "id": "3sFeGeUx-enM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "G_OMoFXK-Zc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/datasets/amazon_reviews_multi\n",
        "dataset = load_dataset(\"amazon_reviews_multi\", \"ja\")"
      ],
      "metadata": {
        "id": "hgrkB4Zt-kMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "oeNR6tyb-ojf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pt1YJEIqlwU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pandasに変換する場合"
      ],
      "metadata": {
        "id": "dJaPZd-h3nQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.set_format(type=\"pandas\")\n",
        "train_df = dataset[\"train\"][:]\n",
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "AxxwoUNb_gEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.reset_format() # データをもとに戻す"
      ],
      "metadata": {
        "id": "f81lbIYq_-Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizerを取得し、トークナイザーのテストをしてみる"
      ],
      "metadata": {
        "id": "5y1DMDGOAFx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "Wq9K46kFAFUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = dataset['train']['review_body'][0]\n",
        "sample_text"
      ],
      "metadata": {
        "id": "vuTjsvwgAJAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text_encoded = tokenizer(sample_text)\n",
        "print(sample_text_encoded)"
      ],
      "metadata": {
        "id": "oHhYKsaGAZwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(sample_text_encoded.input_ids)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "BzevTcAuAdbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_text = tokenizer.convert_tokens_to_string(tokens)\n",
        "print(decode_text)"
      ],
      "metadata": {
        "id": "iPm8ansOAd3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データサイズを削減"
      ],
      "metadata": {
        "id": "4UGrnVnU7evs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データが多いので部分データに変換しておく\n",
        "SEED = 42\n",
        "TRAIN_SIZE = 10000\n",
        "TEST_SIZE = 1000\n",
        "\n",
        "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=SEED).select(range(TRAIN_SIZE))\n",
        "dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=SEED).select(range(TEST_SIZE))\n",
        "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=SEED).select(range(TEST_SIZE))"
      ],
      "metadata": {
        "id": "26jtIgJwAieB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットをトークン化する"
      ],
      "metadata": {
        "id": "as5FViaw76di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def tokenize(batch):\n",
        "    enc =  tokenizer(batch[\"review_body\"], padding=True, truncation=True)\n",
        "    targets = batch['stars']-1\n",
        "    enc.update({'labels': targets})\n",
        "    return enc"
      ],
      "metadata": {
        "id": "GljYaAFAAlqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len = 512\n",
        "# pad_to_max = False\n",
        "# def tokenize_data(example):\n",
        "#     # Tokenize the review body\n",
        "#     text_ = example['review_body'] + \" \" + example['review_title'] + \" \" + example['product_category']\n",
        "#     encodings = tokenizer.encode_plus(text_, pad_to_max_length=pad_to_max, max_length=max_len,\n",
        "#                                            add_special_tokens=True,\n",
        "#                                             return_token_type_ids=False,\n",
        "#                                             return_attention_mask=True,\n",
        "#                                             return_overflowing_tokens=False,\n",
        "#                                             return_special_tokens_mask=False,\n",
        "#                                            )\n",
        "\n",
        "#     # Subtract 1 from labels to have them in range 0-4\n",
        "#     targets = torch.tensor(example['stars']-1,dtype=torch.long)\n",
        "\n",
        "\n",
        "#     encodings.update({'labels': targets})\n",
        "#     return encodings"
      ],
      "metadata": {
        "id": "OQD0Mq4bwJOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(dataset['train'][0]).keys()"
      ],
      "metadata": {
        "id": "GIkZvXavAmAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_encoded = dataset.map(tokenize)"
      ],
      "metadata": {
        "id": "bcpWySfxAnG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xQVLaCyEtRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_encoded[\"train\"][0]['review_body']"
      ],
      "metadata": {
        "id": "mS_a2i1lBicl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "sample_encoded = dataset_encoded[\"train\"][0]\n",
        "pd.DataFrame(\n",
        "    [sample_encoded[\"input_ids\"]\n",
        "     , sample_encoded[\"attention_mask\"]\n",
        "     , tokenizer.convert_ids_to_tokens(sample_encoded[\"input_ids\"])],\n",
        "    ['input_ids', 'attention_mask', \"tokens\"]\n",
        ").T"
      ],
      "metadata": {
        "id": "-Xieb1ymArKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset = dataset_encoded['train']\n",
        "small_valid_dataset = dataset_encoded['validation']\n",
        "small_test_dataset = dataset_encoded['test']"
      ],
      "metadata": {
        "id": "dF3_Us0FA1Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ScuSMQyhkRBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習"
      ],
      "metadata": {
        "id": "kgJ69pOR4UfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_labels = 5\n",
        "\n",
        "model = (AutoModelForSequenceClassification\n",
        "    .from_pretrained(model_ckpt, num_labels=num_labels)\n",
        "    .to(device))"
      ],
      "metadata": {
        "id": "Qkwt-7ICA1wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    preds, labels = pred\n",
        "    preds = preds.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ],
      "metadata": {
        "id": "xIvem2aPA5o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 16\n",
        "logging_steps = len(small_train_dataset) // batch_size\n",
        "model_name = \"amazon-review-classification-bert\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    disable_tqdm=False,\n",
        "    logging_steps=logging_steps,\n",
        "    push_to_hub=False,\n",
        "    log_level=\"error\"\n",
        ")"
      ],
      "metadata": {
        "id": "oljpdb2FA8CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSXrzNqQtz8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_valid_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "HN4AD84EA-6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 混同行列を作成"
      ],
      "metadata": {
        "id": "1H-JVAM_3aU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output = trainer.predict(small_test_dataset)"
      ],
      "metadata": {
        "id": "MN8F9PvX4sx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
        "y_valid = np.array(small_test_dataset[\"labels\"])\n",
        "labels = [\"1star\", \"2star\", \"3star\", \"4star\", \"5star\"]\n",
        "#dataset_encoded[\"train\"].features[\"label\"].names\n",
        "\n",
        "def plot_confusion_matrix(y_preds, y_true, labels):\n",
        "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "    plt.title(\"Normalized confusion matrix\")\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(y_preds, y_valid, labels)"
      ],
      "metadata": {
        "id": "iWWwYk3MBH3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(f\"./{model_name}-test\")"
      ],
      "metadata": {
        "id": "t6XaYoWiBMFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lIA-noPYBOYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 保存したモデルを使って推論してみる"
      ],
      "metadata": {
        "id": "sh1LKoR63TrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokenizer = AutoTokenizer\\\n",
        "    .from_pretrained(f\"./{model_name}-test\")\n",
        "\n",
        "new_model = (AutoModelForSequenceClassification\n",
        "    .from_pretrained(f\"./{model_name}-test\")\n",
        "    .to(device))"
      ],
      "metadata": {
        "id": "vLMirZDpBOmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = new_tokenizer(sample_text, return_tensors=\"pt\")\n",
        "\n",
        "new_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = new_model(\n",
        "        inputs[\"input_ids\"].to(device),\n",
        "        inputs[\"attention_mask\"].to(device),\n",
        "    )\n",
        "\n",
        "print(sample_text)\n",
        "outputs.logits"
      ],
      "metadata": {
        "id": "KiltyeymBQl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = np.argmax(outputs.logits.to('cpu').detach().numpy().copy(), axis=1) + 1\n",
        "y_preds"
      ],
      "metadata": {
        "id": "omYsaUegBZFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}